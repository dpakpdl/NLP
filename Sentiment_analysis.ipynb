{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analyses the two corpuses PA and YT on the basis of association of words with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The [NRC lexicon](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "NRC_EMOTION_LEXICON_PATH = \"Input/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read given input document file and return list of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_file(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    total_lines = list()\n",
    "    for i in doc.paragraphs:\n",
    "        total_lines.append(i.text)\n",
    "    return total_lines\n",
    "filename = 'Input/US3_ALL_TRANSCRIPTS.docx'\n",
    "lines = read_input_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method groups the given text into two groups(PA and YT) and removes unnecessary lines and characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_to_corpuses(lines_in_input):\n",
    "    yt_group = list()\n",
    "    pa_group = list()\n",
    "    initial_group_flag = None\n",
    "    regex = re.compile('^P[0-9]+$')\n",
    "\n",
    "    for line in lines_in_input:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        if line.strip().startswith(\"Joni: \") or line.strip().startswith(\"Jim:\"):\n",
    "            continue\n",
    "        if line.strip().startswith(\"R:\") or line.strip().startswith(\"R: \") or line.strip().startswith(\"R :\"):\n",
    "            continue\n",
    "        if \"R:\" in line:\n",
    "            lines_list = line.splitlines()\n",
    "            for single_line in lines_list:\n",
    "                if single_line.startswith(\"R:\"):\n",
    "                    lines_list.remove(single_line)\n",
    "            line = \",\".join(lines_list)\n",
    "\n",
    "        if re.match(regex, line):\n",
    "            continue\n",
    "        if line.strip().startswith('2018-11-') or line.strip().startswith('Total experiment talk time:'):\n",
    "            continue\n",
    "        line = re.sub(r'\\[[^()]*\\]', '', line)  # regex removing text between brackets\n",
    "\n",
    "        line = line.replace('...', ' ,')\n",
    "        line = line.replace('…', ' ,')\n",
    "\n",
    "        if not line.strip().startswith(\"P:\") and not line.strip().startswith('YT') and not line.strip().startswith(\n",
    "                'PA'):\n",
    "            line = \"\".join([i if ord(i) < 128 else ' ' for i in line])\n",
    "            if line.strip().startswith(\"R :\") or line.strip().startswith(\"R4.\") or line.strip().startswith(\"P10\"):\n",
    "                continue\n",
    "\n",
    "        if line.strip().lower() == 'yt':\n",
    "            initial_group_flag = 'yt'\n",
    "            continue\n",
    "        elif line.strip().lower() == 'pa':\n",
    "            initial_group_flag = 'pa'\n",
    "            continue\n",
    "        elif line:\n",
    "            if initial_group_flag == 'pa':\n",
    "                pa_group.append(line)\n",
    "            else:\n",
    "                yt_group.append(line)\n",
    "    return pa_group, yt_group\n",
    "pa_group, yt_group = group_to_corpuses(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether. Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "pa_group = sent_to_words(pa_group)\n",
    "yt_group = sent_to_words(yt_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemantization of the tokenized words and joining them into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\" \".join(\n",
    "            [token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "pa_group = lemmatization(pa_group)\n",
    "pa_group = \". \".join(pa_group)\n",
    "yt_group = lemmatization(yt_group)\n",
    "yt_group = \". \".join(yt_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitter class use english pickle splitter before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "splitter = Splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech tagging of splitted words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        # adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "postagger = POSTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the NRC emotion lexicon and tag the words with emotion and lexicon. Use this tagged data to find the emotions and sentiment of each word of the POS tagged PA and YT corpus sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictionaryTagger(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.dictionary = dict()\n",
    "        self.max_key_size = 0\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as nrc_file:\n",
    "            for line in nrc_file.readlines():\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                line = re.sub(r'\\s+', '\\t', line)\n",
    "                splited = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "                word, emotion, value = splited[0], splited[1], splited[2]\n",
    "\n",
    "                if word in self.dictionary.keys():\n",
    "                    self.dictionary[word].append((emotion, int(value)))\n",
    "                else:\n",
    "                    self.dictionary[word] = [(emotion, int(value))]\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while i < N:\n",
    "            j = min(i + self.max_key_size, N)  # avoid overflow\n",
    "            tagged = False\n",
    "            while j > i:\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    # self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token:  # if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "    \n",
    "dicttagger = DictionaryTagger(NRC_EMOTION_LEXICON_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_pa_sentences = splitter.split(pa_group)\n",
    "\n",
    "pos_tagged_pa_sentences = postagger.pos_tag(splitted_pa_sentences)\n",
    "\n",
    "dict_tagged_pa_sentences = dicttagger.tag(pos_tagged_pa_sentences)\n",
    "\n",
    "splitted_yt_sentences = splitter.split(yt_group)\n",
    "\n",
    "pos_tagged_yt_sentences = postagger.pos_tag(splitted_yt_sentences)\n",
    "\n",
    "dict_tagged_yt_sentences = dicttagger.tag(pos_tagged_yt_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sentiments and emotions for each word from dictionary tagged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentiment):\n",
    "    if not isinstance(sentiment, tuple):\n",
    "        return dict()\n",
    "    results = dict()\n",
    "    results.update({sentiment[0]: sentiment[1]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the sentiment score and emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(dict_tagged_sentences):\n",
    "    emotions = dict()\n",
    "    for sentence in dict_tagged_sentences:\n",
    "        for token in sentence:\n",
    "            for tag in token[2]:\n",
    "                value = get_sentiment(tag)\n",
    "                if not value:\n",
    "                    continue\n",
    "                emotions.update({tag[0]: emotions.get(tag[0], 0) + value.get(tag[0], 0)})\n",
    "    return emotions\n",
    "\n",
    "pa_sentiment = sentiment_score(dict_tagged_pa_sentences)\n",
    "yt_sentiment = sentiment_score(dict_tagged_yt_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform MannWhitney-U test to compare the emotions and sentiments between PA corpus and YT corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA Sentiment: {'anger': 57, 'anticipation': 353, 'disgust': 202, 'fear': 153, 'joy': 211, 'negative': 167, 'positive': 940, 'sadness': 235, 'surprise': 150, 'trust': 494}\n",
      "YT Sentiment: {'anger': 55, 'anticipation': 461, 'disgust': 92, 'fear': 205, 'joy': 169, 'negative': 122, 'positive': 636, 'sadness': 93, 'surprise': 88, 'trust': 417}\n",
      "Mann Whitney-u Test:\n",
      "MannWhitney U Value: 38.0\n",
      "MannWhitney rho Value: 0.19233653136775436\n"
     ]
    }
   ],
   "source": [
    "def mann_whitney_u_test(group_pa, group_yt):\n",
    "    print(\"PA Sentiment: %s\" % group_pa)\n",
    "    print(\"YT Sentiment: %s\" % group_yt)\n",
    "    print(\"Mann Whitney-u Test:\")\n",
    "    pa_count = list(group_pa.values())\n",
    "    yt_count = list(group_yt.values())\n",
    "    try:\n",
    "        mw_stat, mw_p = mannwhitneyu(pa_count, yt_count)\n",
    "    except ValueError:\n",
    "        mw_stat = -1  # in case of ties, Mann-Whitney cannot rank, and so cannot calculate U\n",
    "        mw_p = -1\n",
    "\n",
    "    print(\"MannWhitney U Value: %s\" % mw_stat)\n",
    "    print(\"MannWhitney rho Value: %s\" % mw_p)\n",
    "mann_whitney_u_test(pa_sentiment, yt_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
